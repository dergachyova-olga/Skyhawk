[data]
# fake_train_data: path to json file containing fake articles for training
# real_train_data: path to json file containing real articles for training
# fake_test_data: path to json file containing fake articles for testing
# real_test_data: path to json file containing real articles for testing
# valid: portion path to from train data used to validate the model
fake_train_data = YOUR_PATH
real_train_data = YOUR_PATH
fake_test_data = YOUR_PATH
real_test_data = YOUR_PATH
valid_size = 0.2

[preprocessing]
# remove_stop_words: indicates if standard stop words (e.g., its, he, is, do...)
# lemmatize: indicates if words should be lemmatized (i.e., reduced to their base)
remove_stop_words = True
lemmatize = True

[features]
# bow: indicates if Bag-of-words feature has to be computed
# tfidf: indicates if TF-IDF feature has to be computed
# word_embeddings: indicates if word embeddings have to be computed
bow = True
tfidf = True
word_embeddings = True

[bow]
# max_features: maximum number of tokens
max_features = 5000

[tfidf]
# max_features: maximum number of tokens
# ngram_n: number n in n-grams used for TF-IDF
max_features = 5000
ngram_n = 3

[word_embeddings]
# path: path to word embeddings pre-trained using word2vec
# max_text_length: maximum number of words in text
path = YOUR_PATH
max_text_length = 500

[model]
# train_NB indicates if Naive Bayes has to be trained
# train_LSTM: indicates if LSTM has to be trained
train_nb = True
train_lstm = True

[lstm]
# lstm_neurons: number of neurons in LSTM layer
# dense_neurons: number of neurons in Dense layer
# epochs: number of training epochs (full iterations)
lstm_neurons = 128
dense_neurons = 64
epochs = 10

[output]
# model_folder: path to a folder to save best trained model
# model_name: name for the best model to save
model_folder = YOUR_PATH
model_name = classification_model
